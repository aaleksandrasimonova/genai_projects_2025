_target_: src.trainer.lora_trainers.LoraTrainer
device: "cuda:0"
cfg_step: 10
log_step: 50
n_epochs: 10
epoch_len: 200
resume_from: null # null or path to the checkpoint dir with *.pth and config.yaml
from_pretrained: null
save_period: 1 # checkpoint each save_period epochs in addition to the best epoch
save_dir: "saved"
seed: 0
max_grad_norm: null